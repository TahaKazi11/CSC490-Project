# CSC490 - KVASIR Kings
# Improving PolypConnect - KVASIR Kings
The lack of curated colon and polyp datasets limits the robustness of computer assisted Diagnosis systems. Therefore, we propose a GAN-based machine learning pipeline to generate large quantities of novel polyp images with corresponding ground-truth mask data. This is completed by training two separate GAN models, those being a pix2pix model for polyp edge image generation and an inpainting model based on polypconnect for the actual polyp image generation.

## Dataset Information:
Two datasets are used for training each of the models. Furthermore, we also utilize a dataset of generated plausible polyp segmentation masks as input into the pix2pix model post-training and as part of polypconnect's pretaining data. The polypconnect model is trained using the HyperKvasir dataset. More specifically, the unlabelled images dataset is used for pretraining, while the segemented images dataset is used for finetuning. The pix2pix model is trained using a augmentation of the HyperKvasir segmented images dataset which consists of realy polyp segmentation masks (from HyperKavsir) and assoicated edge maps generated using the canny edge detection algorithm on the masked real polyp images.
#### HyperKvasir
* [Download URL](https://datasets.simula.no/hyper-kvasir)
* [Paper Reference](https://www.nature.com/articles/s41597-020-00622-y)
Unlabeled Images: 99417 GI-tract images of varying resolution
Segmented Images: 1000 polyp images and respective segmentation mask at varying resolutions

#### Synth. Segmentation Masks
* [Auxilliary Seg Mask Dataset](https://zenodo.org/record/5537151#.Y1b3SEzMKUk)
* [Paper Reference](https://arxiv.org/abs/2106.04463)
10000 synthesized poylp segmentation masks at varying resolution

For examples of the datasets, please refer to the datasets directory.

## PolypConnect:
### Pipeline:
The polypconnect model for inpainting polyps onto colon images is a 4-step pipeline:
<ol>
  <li>The model is pretrained to learn to inpaint general colon images. This is done by giving the model random colon images from
the unlabeled dataset paired with random segmentation masks from the synthetic segmentation mask dataset.</li>
  <li>The model is finetuned with actual polyp images from the segmented images dataset. This step helps the model to learn the 
features specific to polyps in the colon. Note, we apply augmentations to the 800 image training set to improve the robustness 
of the model. For more information, please see the section on data augmentation. </li>
  <li>The input data for novel image generation is prepared. In the original approach, this is done by taking a real polyp image from the segmented images dataset and generating the edges of the polyp by masking the rest of the image out with the ground truth segmentation mask. Once the edges are generated for the given segmentation mask, the pair is combined with a random colon image. We hope to improve the robustness of the overall pipeline by using pix2pix to generate edges for any segmentation mask. This will allow for more variation in the images generated by the final step. </li>
  <li>Finally, the input data created in step 3 is fed into the finetuned inpainting model. The output consists of the novel polyp image
which can be paired with the segmentation mask utilized to serve as ground truth data.</li>
</ol> 

### Prerequisites:
 * *nix 
 * GPU(s)

### Dependencies:
Install the following libraries using your choice of env/pkg manager (i.e. pip or anaconda). Note the installation for torch, torchvision, and CUDA/ROCM support vary widely release to release. For more information please refer to the [PyTorch documentation](https://pytorch.org)
* torch 
* torchvision 
* hydra-core  
* numpy 
* omegaconf 
* pillow 
* scikit-image 
* opencv-python

### Training:
Prior to training, it is a good idea to create a unique configuration file. The simplest way to do this is to create a new folder under the configs directory with a ".conf" extension (used by hydra). Then, copy the config.yaml in in `configs/base.conf/config.yaml` to your newly created directory. You can then edit this file as you see fit for the specifc training instance you are working on. For mor information on the configuration options, please see the comments in the aformentioned config.yaml found at `configs/base.conf/config.yaml`.

Once configured, training can take place. This can be accomplished by running the following command:

`python3 ./edge-connect/main.py --config-path ../configs/<config dir you created>.conf`

NOTE: the --config-path argument is relative to where main.py is which is why the double dot realtive path is used.

Checkpoints are logged to the directory specified by the PATH option in the config.yaml files.


### Testing:
To test the model, some modification to the config.yaml file is required. This largely consits of changing the MODE parameter to 2. Again, command to start testing is:

`python3 ./edge-connect/main.py --config-path ../configs/<config dir you created>.conf`

## Pix2Pix Model:
We used the Pix2Pix model for our image-to-image translation (I2I) method, where we wanted to translate an image of a polyp segmentation mask to into an polyp edge map. We used the Google Colab code they provided for training, we first used a script they provided in order to create paired images of segmentation masks and corresponding edge maps, that were provided, to feed into the model as training input. Finally, we then changed a few lines of code to feed our specific images. Provided below are the Google Colab codes that were used. We planned to use I2I, such that the current pipeline does not rely on real polyp images - unlike in Polypconnect, where Polypconnect needs real image data. After this, the edge map can be used to inpaint a polyp in the healthy colon.

## Pix2Pix Model Information/Code:
* [Pix2Pix Reference](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)
* [Google Colab Code 1](https://colab.research.google.com/drive/1oyL-fmNhVzOxzvKXhHM3B4wFDcGfIuao?usp=sharing) 
* [Google Colab Code 2](https://colab.research.google.com/drive/1vxoApD4cHN2dE7ygtc6gdMeHFSVshpZF?authuser=5#scrollTo=yFw1kDQBx3LN)

## Members:
* Taha Kazi
* Thomas Mayer
* Venura Perera
* Jeffie Wong